import sys
import os
import datetime
import subprocess
import json
import re
import numpy as np
from PIL import Image
import src.problemgenerator.series as series
import src.problemgenerator.array as array
import src.problemgenerator.filters as filters
import src.problemgenerator.copy as copy
from src.combiner.combiner import Combiner
#from src.utils import load_digits_as_npy
from src.utils import load_newsgroups_as_pickle
from src.paramselector.param_selector import ParamSelector
import src.utils as utils
import src.problemgenerator.utils

# File format:
#     run_model_command ...
#     run_analyze_command ...
# special tokens:
#    [IN_i]: Will get replaced with input file i
#    [MID_j.ext]: Will get replaced with unique filename with extension .ext
#                 These files get listed as mid_file_names.
#    [OUT_k.ext]: Will get replaced with unique filename with extension .ext
#                 These files get listed as out_file_names.
#    The same token can appear multiple times. For example, if the commands are
#        python3 model.py < [IN_1] > [MID_1.npy] 2> [MID_2.txt]
#        python3 analyze.py < [MID_1.npy] > [OUT_1.npy]
#    The file given to analyze_py will be the same .npy file as the one generated by model.py
def read_commands_file(commands_file_name):
    f = open(commands_file_name)
    run_model_command = f.readline().rstrip('\n')
    run_analyze_command = f.readline().rstrip('\n')
    return run_model_command, run_analyze_command

def unique_filename(folder, prefix, extension):
    current_folder = os.path.dirname(os.path.realpath(__file__))
    timestamp_string = str(datetime.datetime.utcnow().timestamp())
    fname = f"{current_folder}/{folder}/{prefix}_{timestamp_string}.{extension}"
    return fname

def do_replacements(command, replacements):
    for key, value in replacements.items():
        command = command.replace(key, value)
    return command

def create_replacements(command, token_signature):
    regex = r'\[' + re.escape(token_signature) + r'_(\d*)\.(\w*)\]'
    matches = re.findall(regex, command)
    replacements = {}
    for pr in matches:
        rep_str = "[" + token_signature + "_" + pr[0] + "." + pr[1] + "]"
        tar_str = unique_filename("tmp", token_signature + "-" + str(pr[0]), pr[1])
        replacements[rep_str] = tar_str
    return replacements

def run_commands(run_model_command, run_analyze_command, in_file_names):
    in_replacements = {'[IN_' + str(i+1) + ']': in_file_names[i] for i in range(0, len(in_file_names))}
    mid_replacements = create_replacements(run_model_command + run_analyze_command, "MID")
    out_replacements = create_replacements(run_model_command + run_analyze_command, "OUT")

    run_model_command = do_replacements(run_model_command, in_replacements)
    run_model_command = do_replacements(run_model_command, mid_replacements)
    run_model_command = do_replacements(run_model_command, out_replacements)

    run_analyze_command = do_replacements(run_analyze_command, in_replacements)
    run_analyze_command = do_replacements(run_analyze_command, mid_replacements)
    run_analyze_command = do_replacements(run_analyze_command, out_replacements)

    print(run_model_command)
    print(run_analyze_command)

    subprocess.run(run_model_command, shell=True)
    subprocess.run(run_analyze_command, shell=True)

    mid_file_names = [value for key, value in sorted(mid_replacements.items())]
    out_file_names = [value for key, value in sorted(out_replacements.items())]
    return mid_file_names, out_file_names

def read_analyzer_files(file_names):
    res = []
    for fn in file_names:
        extension = fn.split('.')[-1]
        if extension == 'json':
            with open(fn, "r") as file:
                res.append(json.load(file))
        elif extension == 'png':
            res.append(Image.open(fn))
    return res

def main():
    # Read input files
    path_to_data, path_to_labels, path_to_label_strings = load_newsgroups_as_pickle() # Values 0..16.
    print(path_to_data, path_to_labels, path_to_label_strings)
    original_data_files = [path_to_data, path_to_labels, path_to_label_strings]
    original_data = tuple([np.array(np.load(data_file, allow_pickle=True)[0:1000]) for data_file in original_data_files])
    original_data = tuple([original_data[0].reshape((1000, 1)), original_data[1].reshape((1000, 1)), original_data[2]])

    # Read config for parameter selector
    parsel_config_filename = sys.argv[1]
    error_config_filename = sys.argv[2]
    combiner_config_filename = sys.argv[3]
    model_config_filename = sys.argv[4]

    param_selector = ParamSelector(parsel_config_filename=parsel_config_filename,
                                   model_config_filename=model_config_filename,
                                   error_config_filename=error_config_filename)

    while(param_selector.should_continue()):
        # For parallel processing, take multiple sets of commands here
        run_model_command, run_analyze_command = param_selector.next_commands()

        def save_errorified(ocr_error_prob, ocr_dict):
            print(ocr_error_prob, ocr_dict)
            x_node = array.Array(original_data[0][0].shape)
            x_node.addfilter(filters.OCRError(ocr_dict, p=ocr_error_prob))
            y_node = array.Array(original_data[1][0].shape)
            z_node = array.Array(original_data[2][0].shape)
            series_node = series.TupleSeries([x_node, y_node, z_node])
            error_generator_root = copy.Copy(series_node)
            x_out, y_out, z_out = error_generator_root.process(original_data)
            x_name = unique_filename("tmp", "x", "npy")
            y_name = unique_filename("tmp", "y", "npy")
            z_name = unique_filename("tmp", "z", "npy")
            np.save(x_name, x_out)
            np.save(y_name, y_out)
            np.save(z_name, z_out)
            return [x_name, y_name, z_name]

        # Read error parameters from file (file name given as second argument)
        error_params = json.load(open(error_config_filename))
        ocr_params = error_params['ocr']
        ocr_prob_param = ocr_params['p'] # Iterable of form (start, stop, num)
        ocr_prob_vals = utils.expand_parameter_to_linspace(ocr_prob_param)
        ocr_dict = src.problemgenerator.utils.normalize_ocr_error_params(ocr_params['errors'])

        # Run commands
        combined_file_names = []
        for ocr_prob in ocr_prob_vals:
            err_file_names = save_errorified(ocr_prob, ocr_dict)
            _, out_file_names = run_commands(run_model_command, run_analyze_command, err_file_names)
            combined_file_names.append(({"gaussian" : ocr_prob, "throwaway" : 0.0}, out_file_names))

        # Read input files
        combine_data = [(params, read_analyzer_files(file_names)) for (params, file_names) in combined_file_names]

        param_selector.read_analysis(combine_data)

        # Maybe do not output combined results if param_selector doesn't want to,
        # or combine results over multiple runs
        Combiner.combine(combine_data, output_path="out", config_path=combiner_config_filename)

if __name__ == '__main__':
    main()
